## NYC Taxi Data ETL Pipeline

This project is very simple and it's my first time using Docker. It implements an ETL (Extract, Transform, Load) pipeline for processing NYC taxi trip data. The pipeline extracts data from Parquet files, transforms it using Pandas, and loads it into a PostgreSQL database.

---

### ğŸ—ï¸ Project Structure

```text
etl_taxi/
â”‚
â”œâ”€â”€ data/                   # Data directory with taxi trip Parquet file
â”œâ”€â”€ notebooks/              # Jupyter notebooks for EDA
â”‚   â””â”€â”€ taxi_analysis.ipynb
â”‚
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ extract/            # Data extraction module
â”‚   â”‚   â””â”€â”€ extract_data.py
â”‚   â””â”€â”€ database/           # Database operations
â”‚       â”œâ”€â”€ setup.py
â”‚       â”œâ”€â”€ connection.py
â”‚       â””â”€â”€ load_data.py
â”‚
â”œâ”€â”€ Dockerfile              # Dockerfile for the application
â”œâ”€â”€ docker-compose.yml      # Docker Compose configuration
â”œâ”€â”€ .env.example            # Example environment file
â””â”€â”€ requirements.txt        # Python dependencies

### ğŸš€ Features
Permalink: ğŸš€ Features
- Data extraction from parquet files
- Data transformation and cleaning
- PostgreSQL database integration
- Dockerized application and database
- Modular and maintainable code structure
### ğŸ“Š Data Processing
Permalink: ğŸ“Š Data Processing
The pipeline processes the following data points:
- Pickup and dropoff timestamps
- Passenger count
- Trip distance
- Location IDs
- Payment information
- Fare details
### ğŸ› ï¸ Technical Stack
Permalink: ğŸ› ï¸ Technical Stack
- Python 3.11
- Pandas for data manipulation
- PostgreSQL for data storage
- SQLAlchemy for database operations
- Docker for containerization
- Jupyter Notebooks for analysis

### âš™ï¸ Setup and Installation
Permalink: âš™ï¸ Setup and Installation
1. Clone the repository
	```git clone https://github.com/yourusername/etl_taxi.git
cd etl_taxi
```

Permalink: Configure your environment variables
Create a .env file in the root directory (you can copy from .env.example):
```
POSTGRES_USER=your_username
POSTGRES_PASSWORD=your_password
POSTGRES_DB=your_database_name
DATABASE_URL=postgresql+psycopg2://your_username:your_password@db:5432/your_database_name
```


Permalink: Build and start with Docker
```
docker-compose up --build
```

### ğŸ—„ï¸ Database Schema
Permalink: ğŸ—„ï¸ Database Schema
The transformed data is stored in a table with the following structure:
```CREATE TABLE transformed_taxi_data (
    pickup_datetime TIMESTAMP,
    dropoff_datetime TIMESTAMP,
    passenger_count FLOAT,
    trip_distance FLOAT,
    PULocationID INT,
    DOLocationID INT,
    payment_type TEXT,
    fare_amount FLOAT,
    tip_amount FLOAT,
    total_amount FLOAT
);
```

### ğŸ“ Usage
Permalink: ğŸ“ Usage
*Using Docker
```docker-compose up --build
```
That's it, Docker will handle all dependencies, database setup, and run the ETL pipeline


### ğŸ” Data Analysis
Permalink: ğŸ” Data Analysis
The project includes a Jupyter notebook ( notebooks/taxi_analysis.ipynb) that demonstrates:
- Data exploration
- Cleaning procedures
- Statistical analysis
- Data quality checks
### ğŸ‘¥ Contributing
Permalink: ğŸ‘¥ Contributing
Contributions are welcome! Please feel free to submit a Pull Request.
### ğŸ“§ Contact
Permalink: ğŸ“§ Contact
Jerson Enriquez - https://www.linkedin.com/in/jersonenriquezv/
